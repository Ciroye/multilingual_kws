{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import sox\n",
    "import wave\n",
    "import logging\n",
    "import pydub \n",
    "from pydub.playback import play\n",
    "import time\n",
    "import csv\n",
    "import glob\n",
    "import shutil\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of mp3s in training set:  232975\n"
     ]
    }
   ],
   "source": [
    "clips_path = \"../mozilla_common_voice/clips/\" # english corpus\n",
    "df = pd.read_csv(\"../mozilla_common_voice/train.tsv\", sep=\"\\t\")\n",
    "print(\"number of mp3s in training set: \", df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating a micro dataset of 50 clips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_clips=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def listen_to_sample(df, ix, clips_path):\n",
    "    mp3_path = clips_path + df.iloc[ix].path\n",
    "    clip = pydub.AudioSegment.from_mp3(mp3_path)\n",
    "    play(clip)\n",
    "    return clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = np.random.randint(df.shape[0])\n",
    "listen_to_sample(df=df, ix=sample, clips_path=clips_path)\n",
    "selected_clips.append(sample)\n",
    "print(len(selected_clips))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise ValueError(\"warning: overwrites\")\n",
    "df.iloc[selected_clips].to_csv(\n",
    "    \"selected_clips.tsv\", sep=\"\\t\", quoting=csv.QUOTE_MINIMAL, index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected = pd.read_csv(\"selected_clips.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, r in selected.iterrows():\n",
    "    mp3_path = clips_path + r.path\n",
    "    shutil.copy2(mp3_path, \"./micro_dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, run `word_separator/transcribe_word_time_offsets.py` on `./micro_dataset`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split word on boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected = pd.read_csv(\"selected_clips.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO(MMAZ) lower() and remove punctuation, etc\n",
    "# pasted from https://stackoverflow.com/a/32558749\n",
    "def levenshteinDistance(s1, s2):\n",
    "    if len(s1) > len(s2):\n",
    "        s1, s2 = s2, s1\n",
    "\n",
    "    distances = range(len(s1) + 1)\n",
    "    for i2, c2 in enumerate(s2):\n",
    "        distances_ = [i2+1]\n",
    "        for i1, c1 in enumerate(s1):\n",
    "            if c1 == c2:\n",
    "                distances_.append(distances[i1])\n",
    "            else:\n",
    "                distances_.append(1 + min((distances[i1], distances[i1 + 1], distances_[-1])))\n",
    "        distances = distances_\n",
    "    return distances[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def listen_to_split_on_boundaries(timings, clip):\n",
    "    for ix, timing in enumerate(timings):\n",
    "        word = timing['word']\n",
    "        start_ms = timing['start_time'] * 1000\n",
    "        end_ms = timing['end_time'] * 1000\n",
    "        print(f\"word {ix}: {word}\")\n",
    "        play(clip[start_ms:end_ms])\n",
    "        time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ix, row in selected.iterrows():\n",
    "    print(f\"Sample: {ix}\")\n",
    "    fname = \"./transcriptions/\" + row.path + \".json\"\n",
    "    with open(fname, 'r') as fh:\n",
    "        transcription = json.load(fh)\n",
    "    gt = row.sentence\n",
    "    print(f\"Groundtruth: {gt}\")\n",
    "    text = transcription['transcript']\n",
    "    print(f\"Inference: {text}\")\n",
    "    print(\"Edit Distance:\", levenshteinDistance(gt, text))\n",
    "    clip = listen_to_sample(df=selected, ix=ix, clips_path=clips_path)\n",
    "    listen_to_split_on_boundaries(transcription['timings'], clip)\n",
    "    clear_output()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
